# KoMiniLM
ğŸ’ª Korean light weight language model

## Why
- ì„œë¹„ìŠ¤ ì¸¡ë©´ì—ì„œ í° ìš©ëŸ‰ì„ ê°–ëŠ” ê¸°ì¡´ ì–¸ì–´ ëª¨ë¸ì˜ í•œê³„ë¥¼ ë³´ì™„í•˜ê³ ì ê²½ëŸ‰í™”ëœ ì–¸ì–´ ëª¨ë¸ì„ ê³µê°œí•©ë‹ˆë‹¤.

## Quick tour
`NOTE`: **KoMiniLM** will be released as open source.
```python
from transformers import AutoTokenizer, AutoModel

# KoMiniLM-68M (68M)
tokenizer = AutoTokenizer.from_pretrained("BM-K/")
model = AutoModel.from_pretrained("BM-K/")

# KoMiniLM-23M (23M)
tokenizer = AutoTokenizer.from_pretrained("BM-K/")
model = AutoModel.from_pretrained("BM-K/")

inputs = tokenizer("ì•ˆë…• ì„¸ìƒì•„!", return_tensors="pt")
outputs = model(**inputs)
```

## Pre-training
`Teacher Model`: [KLUE-BERT(base)](https://github.com/KLUE-benchmark/KLUE)
### Object
- Self-Attention Distribution ë° Self-Attention Value-Relation [[Wang et al., 2020]](https://arxiv.org/abs/2002.10957)ì„ êµì‚¬ ëª¨ë¸ì˜ ë¶ˆì—°ì†ì ì¸ ê° ì¸µì—ì„œ í•™ìƒ ëª¨ë¸ë¡œ ì¦ë¥˜í•˜ì˜€ìŠµë‹ˆë‹¤.
### Data set
|ë°ì´í„°|ë‰´ìŠ¤ëŒ“ê¸€|ë‰´ìŠ¤ê¸°ì‚¬|
|:----:|:----:|:----:|
|í¬ê¸°|10G|10G|
### Config
- **KoMiniLM-68M**
```json
{
  "architectures": [
    "BertForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "output_attentions": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "return_dict": false,
  "torch_dtype": "float32",
  "transformers_version": "4.13.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 32000
}
```
- **KoMiniLM-23M**
```json
{
  "architectures": [
    "BertForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 384,
  "initializer_range": 0.02,
  "intermediate_size": 1536,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "output_attentions": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "return_dict": false,
  "torch_dtype": "float32",
  "transformers_version": "4.13.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 32000
}
```
## Performance on subtask
|| #Param | NSMC<br>(Acc) | Naver NER<br>(F1) | PAWS<br>(Acc) | KorNLI<br>(Acc) | KorSTS<br>(Spearman) | Question Pair<br>(Acc) | KorQuaD<br>(Dev)<br>(EM/F1) | 
|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|
|KoBERT(KLUE)| 110M | 90.20Â±0.07 | 87.11Â±0.05 | 81.36Â±0.21 | 81.06Â±0.33 | 82.47Â±0.14 | 95.03Â±0.44 | 84.43Â±0.18 / <br>93.05Â±0.04 |
|KcBERT| 108M | 89.60Â±0.10 | 84.34Â±0.13 | 67.02Â±0.42| 74.17Â±0.52 | 76.57Â±0.51 | 93.97Â±0.27 | 60.87Â±0.27 / <br>85.01Â±0.14 |
|KoBERT(SKT)| 92M | 89.28Â±0.42 | 87.54Â±0.04 | 80.93Â±0.91 | 78.18Â±0.45 | 75.98Â±2.81 | 94.37Â±0.31  | 51.94Â±0.60 / <br>79.69Â±0.66 |
|DistilKoBERT| 28M | 88.39Â±0.08 | 84.22Â±0.01 | 61.74Â±0.45 | 70.22Â±0.14 | 72.11Â±0.27 | 92.65Â±0.16 | 52.52Â±0.48 / <br>76.00Â±0.71 |
|  |  |  |  |  |  |  |  |  |
|**KoMiniLM<sup>â€ </sup>**| **-M** | 89.72 | 85.87 | 80.5 | 79.44 | 80.90 | 94.06 | 82.78 / 91.84|
|**KoMiniLM<sup>v2</sup>**| **23M** | 89.68 | 84.83 | 79.2 | 78.00 | 79.04 | 94.98 | 82.66 / 91.62 |

## ToDo
- [X] An average of 3 runs for each task
- [ ] Training the entire KoMiniLM
- [ ] Huggingface model porting
